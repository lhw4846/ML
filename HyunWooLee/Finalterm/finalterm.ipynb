{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영화 평점 게시글을 이용한 정서 분석\n",
    "\n",
    "## 목차\n",
    "- 가설\n",
    "- 연구방법\n",
    "- 결론\n",
    "\n",
    "## 1. 가설\n",
    "- 나이브 베이즈 분류기와 SVM 분류기를 통해 영화 평점 게시글의 정서를 파악할 수 있다.\n",
    "\n",
    "## 2. 연구방법\n",
    "- 2.1 데이터 수집\n",
    "- 2.2 분류기 생성\n",
    "- 2.3 분류기 평가\n",
    "\n",
    "### 2.1 데이터 수집\n",
    "- Movie Review Data 이용\n",
    "- Sentiment polarity(positive or negative)를 분류함\n",
    "- 5331 positive and 5331 negative processed sentences / snippets. Introduced in Pang/Lee ACL 2005. Released July 2005.\n",
    "- 출처: http://www.cs.cornell.edu/People/pabo/movie-review-data/\n",
    "\n",
    "### 2.2 분류기 생성\n",
    "\n",
    "#### 2.2.1 Load data\n",
    "- Positive post 5331개와 negative post 5331개를 로드함 (rt-polarity.pos, rt-polarity.neg)\n",
    "- 최소 2글자 이상\n",
    "- 소문자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    listOfTokens = bigString.strip().split(' ')\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] \n",
    "\n",
    "def loadDataSet():\n",
    "    # positive\n",
    "    fr=open('data/rt-polarity.pos')\n",
    "    posPosting = [textParse(inst) for inst in fr.readlines()]\n",
    "    posLen = len(posPosting)\n",
    "    posLabels = ones(posLen).tolist()\n",
    "    \n",
    "    # negative\n",
    "    fr=open('data/rt-polarity.neg')\n",
    "    negPosting = [textParse(inst) for inst in fr.readlines()]\n",
    "    negLen = len(negPosting)\n",
    "    negLabels = zeros(negLen).tolist()\n",
    "    \n",
    "    # merge\n",
    "    posPosting.extend(negPosting)\n",
    "    posLabels.extend(negLabels)\n",
    "   \n",
    "    return posPosting, posLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of post list: 10662\n",
      "first post of post list: ['the', 'rock', 'destined', 'the', '21st', \"century's\", 'new', 'conan', 'and', 'that', \"he's\", 'going', 'make', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', 'jean-claud', 'van', 'damme', 'steven', 'segal']\n",
      "first class of class list : 1.0\n"
     ]
    }
   ],
   "source": [
    "listOPosts,listClasses = loadDataSet()\n",
    "print 'length of post list: %d' % len(listOPosts)\n",
    "print 'first post of post list: %s' % listOPosts[0]\n",
    "print 'first class of class list : %s' % listClasses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Naive basis code \n",
    "- 분류기 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        #else: print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec\n",
    "\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() \n",
    "    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num/p1Denom)          #change to log()\n",
    "    p0Vect = log(p0Num/p0Denom)          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.13191048  -8.34015101 -10.53737558 ..., -11.23052277 -10.53737558\n",
      "  -9.8442284 ]\n",
      "[ -9.85111473  -8.52935889 -10.54426191 ..., -10.1387968   -9.85111473\n",
      " -11.23740909]\n",
      "0.5\n",
      "process time is: 10.2923255532\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "trainMat=[]\n",
    "# training\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    \n",
    "start_time = time.clock() # start time\n",
    "\n",
    "p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses)) # training naive basis\n",
    "\n",
    "end_time = time.clock() # end time\n",
    "process_time = end_time - start_time # process time\n",
    "\n",
    "print p0V\n",
    "print p1V\n",
    "print pAb\n",
    "print \"process time is: %s\" % process_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Support vector machine code\n",
    "- 분류기 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import svmutil as svm\n",
    "def trainSVM(trainMatrix, trainCategory):\n",
    "    svm.svm_model.predict = lambda self, x: svm.svm_predict([0], [x], self)[0][0]\n",
    "\n",
    "    prob = svm.svm_problem(trainCategory, trainMatrix)\n",
    "    param = svm.svm_parameter()\n",
    "    param.kernel_type = svm.LINEAR\n",
    "    param.C = 10\n",
    "    \n",
    "    model = svm.svm_train(prob, param)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process time is: 82.7104038715\n"
     ]
    }
   ],
   "source": [
    "listClasses = map(int, listClasses)\n",
    "start_time = time.clock() # start time\n",
    "\n",
    "model = trainSVM(trainMat, listClasses) # training svm\n",
    "\n",
    "end_time = time.clock() # end time\n",
    "process_time = end_time - start_time # process time\n",
    "print \"process time is: %s\" % process_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 분류기 평가\n",
    "\n",
    "#### 2.3.1 Naive basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    # training\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    print p0V, p1V, pAb\n",
    "    # test\n",
    "    errorCount = 0\n",
    "    for testIndex in range(0, len(listOPosts)):\n",
    "        thisDoc = array(setOfWords2Vec(myVocabList, listOPosts[testIndex]))    \n",
    "        if classifyNB(thisDoc,p0V,p1V,pAb) != listClasses[testIndex]:\n",
    "            errorCount += 1\n",
    "    print 'the error rate is:', float(errorCount)/len(listOPosts)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.13191048  -8.34015101 -10.53737558 ..., -11.23052277 -10.53737558\n",
      "  -9.8442284 ] [ -9.85111473  -8.52935889 -10.54426191 ..., -10.1387968   -9.85111473\n",
      " -11.23740909] 0.5\n",
      "the error rate is: 6.3215156631\n",
      "process time is: 144.164912856\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock() # start time\n",
    "\n",
    "testingNB() # test naive basis\n",
    "\n",
    "end_time = time.clock() # end time\n",
    "process_time = end_time - start_time # process time\n",
    "print \"process time is: %s\" % process_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 99.9437% (10656/10662) (classification)\n",
      "process time is: 38.6482202593\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock() # start time\n",
    "\n",
    "svm.svm_predict(listClasses, trainMat, model)\n",
    "\n",
    "end_time = time.clock() # end time\n",
    "process_time = end_time - start_time # process time\n",
    "print \"process time is: %s\" % process_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 결론\n",
    "\n",
    "#### Accuracy\n",
    "- 나이브 베이즈 분류기의 오차율은 약 6%로 확인됨\n",
    "- SVM 분류기의 오차율은 약 0.06%로 확인됨\n",
    "\n",
    "#### Process time\n",
    "- 나이브 베이즈 분류기의 훈련 시간은 10.29초로 확인됨\n",
    "- SVM 분류기의 훈련 시간은 82.71초로 확인됨\n",
    "- 나이브 베이즈 분류기의 분류 시간은 144.16초로 확인됨\n",
    "- SVM 분류기의 분류 시간은 38.64초로 확인됨\n",
    "\n",
    "#### Result\n",
    "- SVM 분류기의 성능이 나이브 베이즈 분류기의 성능보다 뛰어난 것으로 판별됨\n",
    "- 나이브 베이즈 분류기의 훈련 시간이 SVM 분류기의 훈련 시간보다 빠른 것으로 판별됨\n",
    "- SVM 분류기의 분류 시간이 나이브 베이즈 분류기의 분류 시간보다 빠른 것으로 판별됨\n",
    "\n",
    "#### Discussion\n",
    "- SVM 분류기가 나이브 베이즈 분류기보다 훈련 시간은 더 오래 걸리지만, 분류 시간과 정확도는 더 뛰어난 것으로 밝혀짐\n",
    "- 실제 정서 분석을 위해서는 SVM 분류기가 더 적합하다고 판단됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
