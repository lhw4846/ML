{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영화 평점 게시글을 이용한 정서 분석\n",
    "\n",
    "### 목차\n",
    "- 가설\n",
    "- 연구방법\n",
    "- 결론\n",
    "\n",
    "### 1. 가설\n",
    "- 나이브 베이즈 분류기를 통해 영화 평점 게시글의 정서를 파악할 수 있다.\n",
    "\n",
    "### 2. 연구방법\n",
    "- 2.1 데이터 수집\n",
    "- 2.2 분류기 생성\n",
    "- 2.3 분류기 평가\n",
    "\n",
    "#### 2.1 데이터 수집\n",
    "- Movie Review Data 이용\n",
    "- Sentiment polarity(positive or negative)를 분류함\n",
    "- 5331 positive and 5331 negative processed sentences / snippets. Introduced in Pang/Lee ACL 2005. Released July 2005.\n",
    "- 출처: http://www.cs.cornell.edu/People/pabo/movie-review-data/\n",
    "\n",
    "#### 2.2 분류기 생성\n",
    "\n",
    "##### 2.2.1 Load data\n",
    "- Positive post 5331개와 negative post 5331개를 로드함 (rt-polarity.pos, rt-polarity.neg)\n",
    "- 최소 2글자 이상\n",
    "- 소문자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    listOfTokens = bigString.strip().split(' ')\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] \n",
    "\n",
    "def loadDataSet():\n",
    "    # positive\n",
    "    fr=open('data/rt-polarity.pos')\n",
    "    posPosting = [textParse(inst) for inst in fr.readlines()]\n",
    "    posLen = len(posPosting)\n",
    "    posLabels = ones(posLen).tolist()\n",
    "    \n",
    "    # negative\n",
    "    fr=open('data/rt-polarity.neg')\n",
    "    negPosting = [textParse(inst) for inst in fr.readlines()]\n",
    "    negLen = len(negPosting)\n",
    "    negLabels = zeros(negLen).tolist()\n",
    "    \n",
    "    # merge\n",
    "    posPosting.extend(negPosting)\n",
    "    posLabels.extend(negLabels)\n",
    "   \n",
    "    return posPosting, posLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of post list: 10662\n",
      "first post of post list: ['the', 'rock', 'destined', 'the', '21st', \"century's\", 'new', 'conan', 'and', 'that', \"he's\", 'going', 'make', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', 'jean-claud', 'van', 'damme', 'steven', 'segal']\n"
     ]
    }
   ],
   "source": [
    "listOPosts,listClasses = loadDataSet()\n",
    "print 'length of post list: %d' % len(listOPosts)\n",
    "print 'first post of post list: %s' % listOPosts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Naive basis code\n",
    "- 분류기 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        #else: print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec\n",
    "\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() \n",
    "    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num/p1Denom)          #change to log()\n",
    "    p0Vect = log(p0Num/p0Denom)          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.3 분류기 평가\n",
    "- 전체 post에 대하여 분류 결과 확인\n",
    "- 최종 오차율 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    # training\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    # test\n",
    "    errorCount = 0\n",
    "    for testIndex in range(0, len(listOPosts)):\n",
    "        thisDoc = array(setOfWords2Vec(myVocabList, listOPosts[testIndex]))    \n",
    "        if classifyNB(thisDoc,p0V,p1V,pAb) != listClasses[testIndex]:\n",
    "            errorCount += 1\n",
    "    print 'the error rate is:', float(errorCount)/len(listOPosts)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is: 6.3215156631\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 결론\n",
    "- 분류기의 오차율은 약 6%로 확인됨\n",
    "- 가설에 대해 정확도 94%로 기존의 정서 분류기에 비해 양호한 편임\n",
    "- 한계점\n",
    "    - 나이브 베이스 분류기의 특성 상, Training data set에 영향을 많이 받음\n",
    "    - 단어만으로 분류를 하기 때문에, 문맥을 파악하는데 한계를 지님"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
