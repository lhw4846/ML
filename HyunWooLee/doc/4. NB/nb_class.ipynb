{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장 NB(Naive Bayes)\n",
    "\n",
    "\n",
    "### 4.1 Classifying with Bayesian dicision theory\n",
    "\n",
    "#### Pros\n",
    "- Works with a small amount of data, handles multiple classes\n",
    "\n",
    "#### Cons\n",
    "- Sensitive to how the input data is prepared\n",
    "\n",
    "#### Works with\n",
    "- Nominal values\n",
    "\n",
    "\n",
    "### 4.2 Conditional probability\n",
    "- p(c|x) = p(x|c)p(c) / p(x)\n",
    "\n",
    "\n",
    "### 4.3 Classifying with conditional probabilities\n",
    "\n",
    "#### Bayesian decision theory\n",
    "- If p1(x, y) > p2(x, y), then the class is 1\n",
    "- If p2(x, y) > p1(x, y), then the class is 2\n",
    "\n",
    "#### Bayesian classfication rule\n",
    "- p(ci|x,y) = p(x,y|ci)p(ci) / p(x,y)\n",
    "- If P(c1|x,y) > p(c2|x,y), the class is c1\n",
    "- If P(c1|x,y) < p(c2|x,y), the class is c2\n",
    "\n",
    "\n",
    "### 4.4 Document classification with naive Bayes\n",
    "\n",
    "#### General approach to naive Bayes\n",
    "1. Collect: Any method, We'll use RSS feeds in ths chapter.\n",
    "2. Prepare: Numeric or Boolean values are needed.\n",
    "3. Analyze: With many features, plotting features isn't helpful. Looking at histograms is a better idea.\n",
    "4. Train: Calculate the conditional probabilities of the independent features.\n",
    "5. Test: Calculate the error rate.\n",
    "6. Use: One common application of naive Bayes is document classification. You can use naive Bayes in any classification setting. It doesn't have to be text.\n",
    "\n",
    "\n",
    "### 4.5 Classifying text with Python\n",
    "1. we're going to show how to transform lists of text into a vector of numbers\n",
    "2. we'll show how to calculate conditional probabilities from these vectors\n",
    "3. we'll create a classifier\n",
    "4. we'll look at some practical considerations for implementing naive Bayes in Python\n",
    "\n",
    "#### 4.5.1 Prepare: making word vectors from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loadDataSet Function\n",
    "# output\n",
    "#  - postingList: Traning data set\n",
    "#  - classVec: Class labels\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', \\\n",
    "                  'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', \\\n",
    "                  'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', \\\n",
    "                  'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how',\\\n",
    "                  'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1] #1 is abusive, 0 not\n",
    "    return postingList,classVec\n",
    "\n",
    "# createVocabList Function\n",
    "# input\n",
    "#  - dataSet: Training data set\n",
    "# output\n",
    "#  - list(vocabSet): unique vocablary set\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  # create an empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)  # create the union of two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "# setOfWords2Vec Function\n",
    "# input\n",
    "#  - vocabList: Training data Set\n",
    "#  - inputSet: Input data\n",
    "# output\n",
    "#  - returnVec: Vector list of words\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList) # create a vector of all 0s\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loadDataSet example:\n",
      "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "[0, 1, 0, 1, 0, 1]\n",
      "\n",
      "createVocabList example:\n",
      "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n",
      "\n",
      "setOfWords2Vec example:\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# loadDataSet example\n",
    "listOPosts,listClasses = loadDataSet()\n",
    "print \"\\nloadDataSet example:\"\n",
    "print listOPosts\n",
    "print listClasses\n",
    "\n",
    "# createVocabList example\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "print \"\\ncreateVocabList example:\"\n",
    "print myVocabList\n",
    "\n",
    "# setOfWords2Vec example\n",
    "print \"\\nsetOfWords2Vec example:\"\n",
    "print setOfWords2Vec(myVocabList, listOPosts[0])\n",
    "print setOfWords2Vec(myVocabList, listOPosts[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 Train: calculating probabilities from word vectors\n",
    "\n",
    "##### Pseudocode\n",
    "- Count the number of documents in each class\n",
    "- for every training document:\n",
    "    - for each class:\n",
    "        - if a token appears in the document ➞ increment the count for that token\n",
    "        - increment the count for tokens\n",
    "    - for each class:\n",
    "        - for each token:\n",
    "            - divide the token count by the total token count to get conditional probabilities\n",
    "    - return conditional probabilities for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "# trainNB0 function\n",
    "# input\n",
    "#  - trainMatrix: Training data set\n",
    "#  - trainCategory: Class labels\n",
    "# output\n",
    "#  - p0Vect: probability oflabel 0\n",
    "#  - p1Vect: probability oflabel 1\n",
    "#  - pAbusive: probability between labels\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # initialize probabilities\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    p0Num = zeros(numWords); p1Num = zeros(numWords)\n",
    "    p0Denom = 0.0; p1Denom = 0.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # Vector addition\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # Element-wise division\n",
    "    p1Vect = p1Num / p1Denom  # change to log()\n",
    "    p0Vect = p0Num / p0Denom  # change to log()\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainMat:\n",
      "[[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0], [1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]]\n",
      "\n",
      "trainNB0 example:\n",
      "[ 0.04166667  0.04166667  0.04166667  0.          0.          0.04166667\n",
      "  0.04166667  0.04166667  0.          0.04166667  0.04166667  0.04166667\n",
      "  0.04166667  0.          0.          0.08333333  0.          0.\n",
      "  0.04166667  0.          0.04166667  0.04166667  0.          0.04166667\n",
      "  0.04166667  0.04166667  0.          0.04166667  0.          0.04166667\n",
      "  0.04166667  0.125     ]\n",
      "[ 0.          0.          0.          0.05263158  0.05263158  0.          0.\n",
      "  0.          0.05263158  0.05263158  0.          0.          0.\n",
      "  0.05263158  0.05263158  0.05263158  0.05263158  0.05263158  0.\n",
      "  0.10526316  0.          0.05263158  0.05263158  0.          0.10526316\n",
      "  0.          0.15789474  0.          0.05263158  0.          0.          0.        ]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "print \"trainMat:\"\n",
    "print trainMat\n",
    "\n",
    "# trainNB0 example\n",
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "print \"\\ntrainNB0 example:\"\n",
    "print p0V\n",
    "print p1V\n",
    "print pAb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3 Test: modifying the classifier for real-world conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "# trainNB0 function\n",
    "# input\n",
    "#  - trainMatrix: Training data set\n",
    "#  - trainCategory: Class labels\n",
    "# output\n",
    "#  - p0Vect: probability oflabel 0\n",
    "#  - p1Vect: probability oflabel 1\n",
    "#  - pAbusive: probability between labels\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # initialize probabilities\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)\n",
    "    p0Denom = 2.0; p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # Vector addition\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # Element-wise division\n",
    "    p1Vect = log(p1Num / p1Denom)  # change to log()\n",
    "    p0Vect = log(p0Num / p0Denom)  # change to log()\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainMat:\n",
      "[[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0], [1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]]\n",
      "\n",
      "trainNB0 example:\n",
      "[-2.56494936 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936\n",
      " -2.56494936 -3.25809654 -3.25809654 -2.15948425 -3.25809654 -3.25809654\n",
      " -2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -1.87180218]\n",
      "[-3.04452244 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244 -3.04452244\n",
      " -3.04452244 -2.35137526 -2.35137526 -2.35137526 -2.35137526 -2.35137526\n",
      " -3.04452244 -1.94591015 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
      " -1.94591015 -3.04452244 -1.65822808 -3.04452244 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "print \"trainMat:\"\n",
    "print trainMat\n",
    "\n",
    "# trainNB0 example\n",
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "print \"\\ntrainNB0 example:\"\n",
    "print p0V\n",
    "print p1V\n",
    "print pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    # Element-wise multiplication\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def testingNB():\n",
    "    listOPost, listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 Prepare: the bag-of-words document model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "vec = bagOfWords2VecMN(myVocabList, listOPosts[0])\n",
    "print vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Example: classifying spam email with naive Bayes\n",
    "\n",
    "#### Example: using naive Bayes to classify email\n",
    "1. Collect: Test files provided\n",
    "2. Prepare: parse text into token vectors\n",
    "3. Analyze: Inspect the tokens to make sure parsing was done correctly\n",
    "4. Train: Use trainNB0() that we created earlier\n",
    "5. Test: Use classifyNB() and create a new testing function to calculate hte error rate over a set of documents\n",
    "6. Use: Build a complete program that will classify a group of documents and print misclassified documents to the screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 Prepare: tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L.', 'I', 'have', 'ever', 'laid', 'eyes', 'upon.']\n",
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm.l.', 'i', 'have', 'ever', 'laid', 'eyes', 'upon.']\n"
     ]
    }
   ],
   "source": [
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "listOfTokens = mySent.split()\n",
    "print listOfTokens\n",
    "listOfTokens = [tok.lower() for tok in listOfTokens if len(tok) > 0]\n",
    "print listOfTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2 Test: cross validation with naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "def spamTest():\n",
    "    docList=[]; classList=[]; fullText=[]\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open('data/email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('data/email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = range(50); testSet = []\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat = []; trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print 'the error rate is: ', float(errorCount)/len(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.1\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Example: using naive Bayes to reveal local attitudes from personal ads\n",
    "\n",
    "#### Example: using naive Bayes to find locally used words\n",
    "1. Collect: Collect from RSS feeds. We'll need to build an interface to the RSS feeds\n",
    "2. Prepare: Parse text into token vectors\n",
    "3. Analyze: Inspect the tokens to make sure parsing was done correctly\n",
    "4. Train: Use trainNB0() that we created earlier\n",
    "5. Test: We'll look at the error rate to make sure this is actually working. We can make modifications to the tokenizer to improve the error rate and results\n",
    "6. Use: We'll build a complete program to wrap everything together. It will display the most common words given in two RSS feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.1 Collect: importing RSS feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList,fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token]=fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedFreq[:30]\n",
    "\n",
    "def localWords(feed1,feed0):\n",
    "    import feedparser\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    top30Words = calcMostFreq(vocabList,fullText)\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = range(2*minLen); testSet=[]\n",
    "    for i in range(20):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != \\\n",
    "            classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print 'the error rate is: ',float(errorCount)/len(testSet)\n",
    "    return vocabList,p0V,p1V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.2 Analyze: displaying locally used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTopWords(ny, sf):\n",
    "    import operator\n",
    "    vocabList, p0V, p1V = localWords(ny, sf)\n",
    "    topNY=[]; topSF=[]\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i]))\n",
    "        if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print \"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\"\n",
    "    for item in sortedSF:\n",
    "        print item[0]\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print \"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY **\"\n",
    "    for item in sortedNY:\n",
    "        print item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.45\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "lady\n",
      "very\n",
      "enjoy\n",
      "what\n",
      "indian\n",
      "ever\n",
      "give\n",
      "too\n",
      "any\n",
      "find\n",
      "good\n",
      "hang\n",
      "does\n",
      "all\n",
      "whose\n",
      "friendly\n",
      "tired\n",
      "love\n",
      "use\n",
      "sports\n",
      "more\n",
      "share\n",
      "work\n",
      "male\n",
      "how\n",
      "after\n",
      "feels\n",
      "age\n",
      "seeking\n",
      "doing\n",
      "free\n",
      "lol\n",
      "open\n",
      "marriage\n",
      "bedroom\n",
      "busy\n",
      "unhappy\n",
      "husband\n",
      "attractive\n",
      "chill\n",
      "will\n",
      "yes\n",
      "happiness\n",
      "lets\n",
      "back\n",
      "curious\n",
      "single\n",
      "evening\n",
      "other\n",
      "nice\n",
      "pillows\n",
      "oral\n",
      "everything\n",
      "brownie\n",
      "minded\n",
      "join\n",
      "try\n",
      "someday\n",
      "drinks\n",
      "giving\n",
      "learned\n",
      "water\n",
      "great\n",
      "makes\n",
      "thats\n",
      "ask\n",
      "music\n",
      "type\n",
      "separated\n",
      "started\n",
      "company\n",
      "downtown\n",
      "town\n",
      "divorced\n",
      "movies\n",
      "meet\n",
      "excuses\n",
      "foodie\n",
      "high\n",
      "something\n",
      "thing\n",
      "vibe\n",
      "disappoint\n",
      "advice\n",
      "adventure\n",
      "guys\n",
      "man\n",
      "harley\n",
      "pleasure\n",
      "over\n",
      "still\n",
      "fit\n",
      "interesting\n",
      "32ish\n",
      "now\n",
      "wont\n",
      "friendship\n",
      "sane\n",
      "could\n",
      "conversationalist\n",
      "desi\n",
      "soda\n",
      "feel\n",
      "spiritual\n",
      "tomorrow\n",
      "boring\n",
      "tonight\n",
      "livermore\n",
      "ideas\n",
      "useless\n",
      "rides\n",
      "hubby\n",
      "face\n",
      "points\n",
      "wind\n",
      "clean\n",
      "26ish\n",
      "going\n",
      "international\n",
      "him\n",
      "ways\n",
      "please\n",
      "between\n",
      "stories\n",
      "reading\n",
      "right\n",
      "were\n",
      "job\n",
      "coffee\n",
      "restaurant\n",
      "sweet\n",
      "period\n",
      "wants\n",
      "obese\n",
      "life\n",
      "educated\n",
      "look\n",
      "while\n",
      "surprised\n",
      "fun\n",
      "engaged\n",
      "adventures\n",
      "optimistic\n",
      "things\n",
      "make\n",
      "same\n",
      "extended\n",
      "well\n",
      "spent\n",
      "rest\n",
      "thanks\n",
      "unique\n",
      "easy\n",
      "has\n",
      "balloons\n",
      "take\n",
      "hunt\n",
      "born\n",
      "word\n",
      "lasts\n",
      "anything\n",
      "drama\n",
      "ladies\n",
      "eaten\n",
      "intellectual\n",
      "down\n",
      "there\n",
      "bored\n",
      "chat\n",
      "girlfriend\n",
      "moves\n",
      "selfish\n",
      "having\n",
      "loyal\n",
      "hello\n",
      "crossdressing\n",
      "doujins\n",
      "sleep\n",
      "asian\n",
      "skin\n",
      "issues\n",
      "recharge\n",
      "relationships\n",
      "technique\n",
      "father\n",
      "young\n",
      "send\n",
      "charge\n",
      "stable\n",
      "smile\n",
      "include\n",
      "activities\n",
      "collaborate\n",
      "sitting\n",
      "fat\n",
      "anonymous\n",
      "difference\n",
      "cool\n",
      "school\n",
      "magic\n",
      "hour\n",
      "stiff\n",
      "list\n",
      "race\n",
      "guy\n",
      "ride\n",
      "cookies\n",
      "says\n",
      "ten\n",
      "japanese\n",
      "erotic\n",
      "bake\n",
      "even\n",
      "sub\n",
      "section\n",
      "brief\n",
      "emotions\n",
      "goes\n",
      "reply\n",
      "seeks\n",
      "body\n",
      "outgoing\n",
      "full\n",
      "makeup\n",
      "here\n",
      "lgbt\n",
      "explore\n",
      "let\n",
      "others\n",
      "along\n",
      "change\n",
      "kids\n",
      "changed\n",
      "experience\n",
      "smoke\n",
      "stressed\n",
      "weird\n",
      "spices\n",
      "via\n",
      "plea\n",
      "family\n",
      "highly\n",
      "crisis\n",
      "proceed\n",
      "dude\n",
      "two\n",
      "live\n",
      "themselves\n",
      "sessions\n",
      "waterways\n",
      "phone\n",
      "hold\n",
      "women\n",
      "animals\n",
      "challenge\n",
      "soul\n",
      "14yrs\n",
      "entertainer\n",
      "grimes\n",
      "beautiful\n",
      "india\n",
      "foods\n",
      "woman\n",
      "want\n",
      "dress\n",
      "trustworthy\n",
      "write\n",
      "amazing\n",
      "comedy\n",
      "staying\n",
      "description\n",
      "aspiring\n",
      "bodyrub\n",
      "such\n",
      "response\n",
      "stress\n",
      "conscious\n",
      "light\n",
      "profesionnal\n",
      "rather\n",
      "truck\n",
      "typical\n",
      "cute\n",
      "producer\n",
      "mainly\n",
      "soon\n",
      "years\n",
      "through\n",
      "looks\n",
      "committed\n",
      "hell\n",
      "its\n",
      "perfect\n",
      "group\n",
      "writing\n",
      "better\n",
      "wellness\n",
      "then\n",
      "them\n",
      "truest\n",
      "break\n",
      "they\n",
      "hands\n",
      "front\n",
      "thus\n",
      "day\n",
      "maintaining\n",
      "respectful\n",
      "always\n",
      "drop\n",
      "fitness\n",
      "found\n",
      "concerts\n",
      "bond\n",
      "everyone\n",
      "mental\n",
      "weight\n",
      "idea\n",
      "conventions\n",
      "year\n",
      "our\n",
      "girl\n",
      "beyond\n",
      "special\n",
      "living\n",
      "rubdown\n",
      "health\n",
      "hill\n",
      "internet\n",
      "got\n",
      "shows\n",
      "assured\n",
      "driver\n",
      "little\n",
      "quite\n",
      "houses\n",
      "reason\n",
      "sbf\n",
      "york\n",
      "besides\n",
      "put\n",
      "craving\n",
      "sbm\n",
      "benefits\n",
      "created\n",
      "where\n",
      "boerum\n",
      "conversation\n",
      "iso\n",
      "first\n",
      "safety\n",
      "owned\n",
      "hearing\n",
      "hook\n",
      "dominating\n",
      "another\n",
      "sober\n",
      "righ\n",
      "40yrs\n",
      "porn\n",
      "anyone\n",
      "their\n",
      "white\n",
      "friend\n",
      "toe\n",
      "interests\n",
      "eyes\n",
      "way\n",
      "relationship\n",
      "appreciates\n",
      "exactly\n",
      "part\n",
      "copy\n",
      "haven\n",
      "boyfriend\n",
      "photograph\n",
      "kindly\n",
      "see\n",
      "project\n",
      "hispanic\n",
      "maybe\n",
      "listen\n",
      "gathering\n",
      "fashion\n",
      "ages\n",
      "say\n",
      "comedian\n",
      "need\n",
      "hentai\n",
      "seek\n",
      "sell\n",
      "afraid\n",
      "built\n",
      "dancing\n",
      "self\n",
      "able\n",
      "equipment\n",
      "relatives\n",
      "potential\n",
      "online\n",
      "objective\n",
      "wanting\n",
      "gym\n",
      "channel\n",
      "buff\n",
      "play\n",
      "experienced\n",
      "reach\n",
      "visa\n",
      "experiences\n",
      "plenty\n",
      "tall\n",
      "nothing\n",
      "why\n",
      "queens\n",
      "singer\n",
      "don\n",
      "grams\n",
      "sometimes\n",
      "dog\n",
      "pic\n",
      "professional\n",
      "stoner\n",
      "gentleman\n",
      "planning\n",
      "earth\n",
      "one\n",
      "bff\n",
      "title\n",
      "enough\n",
      "eat\n",
      "moments\n",
      "only\n",
      "pretty\n",
      "circle\n",
      "local\n",
      "hope\n",
      "his\n",
      "watching\n",
      "assistant\n",
      "watch\n",
      "kind\n",
      "leisure\n",
      "degradation\n",
      "unimportant\n",
      "paddling\n",
      "rapper\n",
      "borrow\n",
      "bay\n",
      "bad\n",
      "she\n",
      "release\n",
      "grad\n",
      "motorcycle\n",
      "art\n",
      "mind\n",
      "depends\n",
      "computer\n",
      "college\n",
      "close\n",
      "best\n",
      "stats\n",
      "movie\n",
      "currently\n",
      "horny\n",
      "state\n",
      "closed\n",
      "sorts\n",
      "email\n",
      "available\n",
      "fir\n",
      "reddit\n",
      "come\n",
      "both\n",
      "post\n",
      "brown\n",
      "many\n",
      "shouldn\n",
      "against\n",
      "became\n",
      "figures\n",
      "inward\n",
      "poverty\n",
      "com\n",
      "gratification\n",
      "tone\n",
      "point\n",
      "wall\n",
      "each\n",
      "whatever\n",
      "laugh\n",
      "respect\n",
      "blink\n",
      "late\n",
      "decent\n",
      "fro\n",
      "add\n",
      "been\n",
      "much\n",
      "judging\n",
      "relaxing\n",
      "league\n",
      "turned\n",
      "new\n",
      "else\n",
      "instructor\n",
      "those\n",
      "myself\n",
      "situation\n",
      "balance\n",
      "site\n",
      "strain\n",
      "cant\n",
      "ready\n",
      "partner\n",
      "everyday\n",
      "starcraft\n",
      "different\n",
      "doctor\n",
      "confidant\n",
      "mangas\n",
      "exhibitionists\n",
      "trans\n",
      "assignment\n",
      "drink\n",
      "kik\n",
      "sisters\n",
      "totally\n",
      "butter\n",
      "hearth\n",
      "off\n",
      "older\n",
      "know\n",
      "tho\n",
      "person\n",
      "without\n",
      "claims\n",
      "avaiable\n",
      "spend\n",
      "left\n",
      "being\n",
      "money\n",
      "mid\n",
      "touch\n",
      "yet\n",
      "weekly\n",
      "also\n",
      "thinking\n",
      "seems\n",
      "chest\n",
      "simple\n",
      "interested\n",
      "location\n",
      "smart\n",
      "real\n",
      "big\n",
      "couple\n",
      "progressed\n",
      "possible\n",
      "ruff\n",
      "early\n",
      "possibly\n",
      "marcy\n",
      "using\n",
      "bit\n",
      "passable\n",
      "gift\n",
      "gyms\n",
      "fwb\n",
      "lose\n",
      "chick\n",
      "buddies\n",
      "old\n",
      "deal\n",
      "people\n",
      "spring\n",
      "30s\n",
      "clubs\n",
      "escape\n",
      "guess\n",
      "personnal\n",
      "4chan\n",
      "cannabis\n",
      "420\n",
      "business\n",
      "300\n",
      "bowling\n",
      "feelings\n",
      "ple\n",
      "become\n",
      "erotica\n",
      "stone\n",
      "lifestyle\n",
      "getting\n",
      "freedom\n",
      "dinner\n",
      "must\n",
      "anime\n",
      "slightly\n",
      "own\n",
      "into\n",
      "skilled\n",
      "son\n",
      "because\n",
      "involvement\n",
      "tiny\n",
      "legends\n",
      "her\n",
      "area\n",
      "strictly\n",
      "long\n",
      "avail\n",
      "start\n",
      "lonely\n",
      "lot\n",
      "call\n",
      "was\n",
      "happy\n",
      "head\n",
      "complete\n",
      "defrost\n",
      "jus\n",
      "nassau\n",
      "line\n",
      "level\n",
      "dull\n",
      "handsome\n",
      "made\n",
      "romantic\n",
      "wish\n",
      "attached\n",
      "intern\n",
      "limit\n",
      "problem\n",
      "called\n",
      "darker\n",
      "600\n",
      "yourse\n",
      "together\n",
      "home\n",
      "film\n",
      "yoga\n",
      "dying\n",
      "lately\n",
      "surreal\n",
      "ambitious\n",
      "spilli\n",
      "picture\n",
      "gifted\n",
      "students\n",
      "diet\n",
      "herbs\n",
      "drag\n",
      "fell\n",
      "diablo\n",
      "song\n",
      "night\n",
      "studio\n",
      "blogs\n",
      "wellbeing\n",
      "daily\n",
      "text\n",
      "had\n",
      "once\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY **\n",
      "there\n",
      "male\n",
      "things\n",
      "escape\n",
      "let\n",
      "live\n",
      "want\n",
      "age\n",
      "see\n",
      "will\n",
      "take\n",
      "together\n",
      "all\n",
      "young\n",
      "very\n",
      "erotic\n",
      "even\n",
      "body\n",
      "here\n",
      "type\n",
      "woman\n",
      "write\n",
      "bodyrub\n",
      "stress\n",
      "years\n",
      "hell\n",
      "seeking\n",
      "day\n",
      "year\n",
      "living\n",
      "could\n",
      "conversation\n",
      "first\n",
      "white\n",
      "way\n",
      "only\n",
      "hope\n",
      "his\n",
      "watch\n",
      "borrow\n",
      "she\n",
      "against\n",
      "inward\n",
      "poverty\n",
      "wall\n",
      "look\n",
      "partner\n",
      "make\n",
      "well\n",
      "left\n",
      "weekly\n",
      "marcy\n",
      "using\n",
      "old\n",
      "people\n",
      "back\n",
      "her\n",
      "yoga\n",
      "studio\n",
      "having\n",
      "hello\n",
      "doujins\n",
      "recharge\n",
      "technique\n",
      "father\n",
      "charge\n",
      "stable\n",
      "include\n",
      "activities\n",
      "sitting\n",
      "difference\n",
      "school\n",
      "magic\n",
      "stiff\n",
      "race\n",
      "guy\n",
      "says\n",
      "ten\n",
      "japanese\n",
      "sub\n",
      "section\n",
      "brief\n",
      "reply\n",
      "seeks\n",
      "full\n",
      "explore\n",
      "change\n",
      "kids\n",
      "changed\n",
      "experience\n",
      "stressed\n",
      "weird\n",
      "via\n",
      "love\n",
      "highly\n",
      "crisis\n",
      "use\n",
      "two\n",
      "music\n",
      "sessions\n",
      "company\n",
      "waterways\n",
      "hold\n",
      "women\n",
      "challenge\n",
      "soul\n",
      "movies\n",
      "beautiful\n",
      "give\n",
      "thing\n",
      "description\n",
      "such\n",
      "conscious\n",
      "rather\n",
      "pleasure\n",
      "typical\n",
      "cute\n",
      "over\n",
      "mainly\n",
      "looks\n",
      "committed\n",
      "still\n",
      "perfect\n",
      "group\n",
      "writing\n",
      "better\n",
      "then\n",
      "truest\n",
      "break\n",
      "hands\n",
      "front\n",
      "now\n",
      "drop\n",
      "fitness\n",
      "found\n",
      "concerts\n",
      "bond\n",
      "mental\n",
      "weight\n",
      "idea\n",
      "conventions\n",
      "beyond\n",
      "special\n",
      "rubdown\n",
      "hill\n",
      "internet\n",
      "shows\n",
      "assured\n",
      "york\n",
      "craving\n",
      "benefits\n",
      "created\n",
      "boerum\n",
      "safety\n",
      "owned\n",
      "dominating\n",
      "sober\n",
      "righ\n",
      "porn\n",
      "friend\n",
      "interests\n",
      "eyes\n",
      "relationship\n",
      "exactly\n",
      "copy\n",
      "haven\n",
      "boyfriend\n",
      "photograph\n",
      "kindly\n",
      "maybe\n",
      "gathering\n",
      "ages\n",
      "say\n",
      "hentai\n",
      "any\n",
      "sell\n",
      "built\n",
      "self\n",
      "equipment\n",
      "potential\n",
      "online\n",
      "gym\n",
      "play\n",
      "experienced\n",
      "nothing\n",
      "why\n",
      "queens\n",
      "don\n",
      "sometimes\n",
      "pic\n",
      "professional\n",
      "stoner\n",
      "planning\n",
      "find\n",
      "one\n",
      "bff\n",
      "title\n",
      "eat\n",
      "moments\n",
      "going\n",
      "pretty\n",
      "local\n",
      "degradation\n",
      "unimportant\n",
      "paddling\n",
      "bad\n",
      "release\n",
      "computer\n",
      "close\n",
      "horny\n",
      "closed\n",
      "email\n",
      "reddit\n",
      "right\n",
      "job\n",
      "coffee\n",
      "come\n",
      "shouldn\n",
      "figures\n",
      "gratification\n",
      "whatever\n",
      "blink\n",
      "decent\n",
      "been\n",
      "attractive\n",
      "life\n",
      "relaxing\n",
      "league\n",
      "turned\n",
      "new\n",
      "else\n",
      "instructor\n",
      "fun\n",
      "situation\n",
      "strain\n",
      "good\n",
      "ready\n",
      "everyday\n",
      "starcraft\n",
      "different\n",
      "mangas\n",
      "exhibitionists\n",
      "totally\n",
      "hearth\n",
      "older\n",
      "tho\n",
      "without\n",
      "being\n",
      "money\n",
      "mid\n",
      "thanks\n",
      "touch\n",
      "yet\n",
      "also\n",
      "seems\n",
      "location\n",
      "smart\n",
      "real\n",
      "couple\n",
      "progressed\n",
      "early\n",
      "possibly\n",
      "gyms\n",
      "fwb\n",
      "lose\n",
      "chick\n",
      "buddies\n",
      "spring\n",
      "30s\n",
      "curious\n",
      "4chan\n",
      "300\n",
      "ple\n",
      "become\n",
      "erotica\n",
      "stone\n",
      "freedom\n",
      "drama\n",
      "dinner\n",
      "must\n",
      "anime\n",
      "own\n",
      "into\n",
      "skilled\n",
      "son\n",
      "involvement\n",
      "legends\n",
      "strictly\n",
      "lonely\n",
      "call\n",
      "was\n",
      "defrost\n",
      "nassau\n",
      "line\n",
      "level\n",
      "handsome\n",
      "made\n",
      "romantic\n",
      "limit\n",
      "called\n",
      "darker\n",
      "600\n",
      "yourse\n",
      "single\n",
      "film\n",
      "dying\n",
      "lately\n",
      "surreal\n",
      "other\n",
      "ambitious\n",
      "spilli\n",
      "students\n",
      "diablo\n",
      "song\n",
      "night\n",
      "blogs\n",
      "daily\n",
      "text\n",
      "had\n",
      "oral\n",
      "crossdressing\n",
      "sleep\n",
      "asian\n",
      "skin\n",
      "issues\n",
      "relationships\n",
      "whose\n",
      "everything\n",
      "send\n",
      "smile\n",
      "friendly\n",
      "collaborate\n",
      "brownie\n",
      "fat\n",
      "minded\n",
      "anonymous\n",
      "join\n",
      "cool\n",
      "hour\n",
      "list\n",
      "try\n",
      "ride\n",
      "enjoy\n",
      "someday\n",
      "cookies\n",
      "tired\n",
      "bake\n",
      "drinks\n",
      "what\n",
      "giving\n",
      "emotions\n",
      "indian\n",
      "goes\n",
      "learned\n",
      "ever\n",
      "outgoing\n",
      "makeup\n",
      "water\n",
      "lgbt\n",
      "others\n",
      "along\n",
      "great\n",
      "smoke\n",
      "spices\n",
      "makes\n",
      "thats\n",
      "plea\n",
      "family\n",
      "ask\n",
      "proceed\n",
      "dude\n",
      "sports\n",
      "themselves\n",
      "more\n",
      "separated\n",
      "started\n",
      "share\n",
      "downtown\n",
      "phone\n",
      "town\n",
      "animals\n",
      "divorced\n",
      "work\n",
      "14yrs\n",
      "entertainer\n",
      "meet\n",
      "grimes\n",
      "excuses\n",
      "foodie\n",
      "india\n",
      "high\n",
      "foods\n",
      "something\n",
      "dress\n",
      "trustworthy\n",
      "vibe\n",
      "how\n",
      "amazing\n",
      "disappoint\n",
      "comedy\n",
      "staying\n",
      "aspiring\n",
      "advice\n",
      "after\n",
      "adventure\n",
      "guys\n",
      "response\n",
      "man\n",
      "light\n",
      "profesionnal\n",
      "feels\n",
      "harley\n",
      "truck\n",
      "producer\n",
      "soon\n",
      "through\n",
      "its\n",
      "fit\n",
      "interesting\n",
      "wellness\n",
      "them\n",
      "32ish\n",
      "they\n",
      "thus\n",
      "maintaining\n",
      "respectful\n",
      "wont\n",
      "always\n",
      "friendship\n",
      "everyone\n",
      "doing\n",
      "our\n",
      "girl\n",
      "sane\n",
      "health\n",
      "got\n",
      "driver\n",
      "little\n",
      "free\n",
      "quite\n",
      "houses\n",
      "reason\n",
      "sbf\n",
      "besides\n",
      "put\n",
      "sbm\n",
      "where\n",
      "conversationalist\n",
      "desi\n",
      "iso\n",
      "soda\n",
      "feel\n",
      "spiritual\n",
      "hearing\n",
      "hook\n",
      "lol\n",
      "another\n",
      "open\n",
      "tomorrow\n",
      "40yrs\n",
      "anyone\n",
      "their\n",
      "too\n",
      "toe\n",
      "appreciates\n",
      "part\n",
      "boring\n",
      "project\n",
      "hispanic\n",
      "marriage\n",
      "listen\n",
      "fashion\n",
      "tonight\n",
      "comedian\n",
      "need\n",
      "seek\n",
      "livermore\n",
      "afraid\n",
      "dancing\n",
      "able\n",
      "ideas\n",
      "relatives\n",
      "useless\n",
      "objective\n",
      "wanting\n",
      "rides\n",
      "channel\n",
      "buff\n",
      "reach\n",
      "visa\n",
      "experiences\n",
      "plenty\n",
      "tall\n",
      "singer\n",
      "hubby\n",
      "grams\n",
      "dog\n",
      "face\n",
      "points\n",
      "wind\n",
      "clean\n",
      "gentleman\n",
      "26ish\n",
      "bedroom\n",
      "earth\n",
      "busy\n",
      "enough\n",
      "unhappy\n",
      "circle\n",
      "watching\n",
      "assistant\n",
      "kind\n",
      "leisure\n",
      "international\n",
      "him\n",
      "rapper\n",
      "bay\n",
      "grad\n",
      "husband\n",
      "motorcycle\n",
      "art\n",
      "mind\n",
      "depends\n",
      "college\n",
      "best\n",
      "stats\n",
      "ways\n",
      "movie\n",
      "currently\n",
      "please\n",
      "state\n",
      "between\n",
      "stories\n",
      "sorts\n",
      "reading\n",
      "available\n",
      "fir\n",
      "were\n",
      "both\n",
      "post\n",
      "brown\n",
      "restaurant\n",
      "many\n",
      "became\n",
      "com\n",
      "tone\n",
      "point\n",
      "sweet\n",
      "each\n",
      "period\n",
      "laugh\n",
      "respect\n",
      "late\n",
      "fro\n",
      "add\n",
      "much\n",
      "judging\n",
      "wants\n",
      "obese\n",
      "educated\n",
      "those\n",
      "chill\n",
      "myself\n",
      "while\n",
      "surprised\n",
      "balance\n",
      "engaged\n",
      "site\n",
      "adventures\n",
      "cant\n",
      "optimistic\n",
      "doctor\n",
      "confidant\n",
      "same\n",
      "trans\n",
      "extended\n",
      "assignment\n",
      "drink\n",
      "hang\n",
      "kik\n",
      "sisters\n",
      "butter\n",
      "off\n",
      "spent\n",
      "know\n",
      "person\n",
      "claims\n",
      "avaiable\n",
      "spend\n",
      "rest\n",
      "yes\n",
      "happiness\n",
      "unique\n",
      "thinking\n",
      "chest\n",
      "simple\n",
      "lets\n",
      "interested\n",
      "easy\n",
      "has\n",
      "balloons\n",
      "big\n",
      "possible\n",
      "ruff\n",
      "bit\n",
      "passable\n",
      "lady\n",
      "gift\n",
      "hunt\n",
      "deal\n",
      "born\n",
      "clubs\n",
      "guess\n",
      "personnal\n",
      "cannabis\n",
      "does\n",
      "420\n",
      "business\n",
      "word\n",
      "bowling\n",
      "feelings\n",
      "lasts\n",
      "lifestyle\n",
      "anything\n",
      "getting\n",
      "slightly\n",
      "ladies\n",
      "eaten\n",
      "intellectual\n",
      "down\n",
      "because\n",
      "tiny\n",
      "area\n",
      "long\n",
      "avail\n",
      "start\n",
      "lot\n",
      "bored\n",
      "happy\n",
      "head\n",
      "complete\n",
      "jus\n",
      "dull\n",
      "wish\n",
      "attached\n",
      "intern\n",
      "problem\n",
      "evening\n",
      "chat\n",
      "home\n",
      "girlfriend\n",
      "moves\n",
      "selfish\n",
      "nice\n",
      "picture\n",
      "gifted\n",
      "pillows\n",
      "diet\n",
      "herbs\n",
      "drag\n",
      "fell\n",
      "wellbeing\n",
      "loyal\n",
      "once\n"
     ]
    }
   ],
   "source": [
    "ny=feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf=feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "getTopWords(ny,sf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
